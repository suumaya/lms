<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">This Week in JBoss - 08 October 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/HzlcUfLTv5g/weekly-2021-10-08.html" /><category term="quarkus" /><category term="kogito" /><category term="infinispan" /><category term="vert.x" /><category term="java" /><category term="wildfly" /><category term="elytron" /><category term="jbang" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-10-08.html</id><updated>2021-10-08T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kogito, infinispan, vert.x, java, wildfly, elytron, jbang"&gt; &lt;h1&gt;This Week in JBoss - 08 October 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Congrats on all the releases and great work from the community!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-3-0-final-released/"&gt;Quarkus 2.3.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-5/"&gt;Eclipse Vert.x 4.1.5&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2021/09/23/infinispan-13-cr1"&gt;Infinispan 13.0.0.CR1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_wildfly_25"&gt;WildFly 25&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/10/05/WildFly25-Final-Released/"&gt;WildFly 25 is released!&lt;/a&gt;, by Brian Stansberry&lt;/p&gt; &lt;p&gt;It feels appropriate to call special attention to Brian’s release announcement of WildFly 15. The entire team has done great work and Brian goes into detail about all the new features, including finishing the migration to Elytron-based security and Java SE 17 support. Congrats to the WildFly community on another fantastic milestone!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_elytron_hacktoberfest"&gt;Elytron Hacktoberfest&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/hacktoberfest-has-begun/"&gt;Hacktoberfest Has Begun&lt;/a&gt;, by Farah Juma&lt;/p&gt; &lt;p&gt;Calling all hackers!! WildFly Elytron is participating in this year’s Hacktoberfest. Check out Farah’s post to register and contribute to Elytron, Elytron Web, or WildFly OpenSSL repositories.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_run_decision_manager_with_podman"&gt;Run Decision Manager with Podman&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.schabell.org/2021/10/beginners-guide-to-rhdm-local-conainter-podman.html.html"&gt;Beginners Guide to Installing Decision Management Tooling in a Local Container using Podman&lt;/a&gt;, by Eric Schabell&lt;/p&gt; &lt;p&gt;Eric’s step-by-step tutorial taught me how to quickly set up a running instance of Decision Manager in a local container. I really enjoyed the tutorial and would recommend as a great way to introduce yourself to the world of process automation.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_authenticating_to_business_central_with_ssh_keys"&gt;Authenticating to Business Central with SSH keys&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/10/business-central-ssh-key-based-authentication.html"&gt;Business Central SSH Key-Based Authentication&lt;/a&gt;, by Eder Ignatowicz&lt;/p&gt; &lt;p&gt;This brief article shows you how easy it is to configure authentication with Business Central using SSH keys. At the end, Eder also combines JBang with JGit in a nifty way to use a different SSH keys and provides you with the code.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus_insights"&gt;Quarkus Insights&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some Quarkus insights. Here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/2cHX8HT7Z6A"&gt;Quarkus Insights #66: SNCF migration from WildFly to Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/efPuzK2YM4I"&gt;Quarkus Insights #65: Boost Business Automation Productivity with Quarkus Dev UI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/HzlcUfLTv5g" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-10-08.html</feedburner:origLink></entry><entry><title type="html">Effective data generation for explaining decision services</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZjJ4BMokKIM/effective-data-generation-for-explaining-decision-services.html" /><author><name>Tommaso Teofili</name></author><id>https://blog.kie.org/2021/10/effective-data-generation-for-explaining-decision-services.html</id><updated>2021-10-07T22:35:07Z</updated><content type="html">When working with decision services, it’s often hard to understand the rationale behind the output for a given prediction. A noteworthy example is the one related to having a decision service for loan approval denying the loan request to a given user. The user would surely like to know the rationale behind such a denial. Within TrustyAI initiative we developed an optimized implementation of the LIME (aka ) that is well suited for the decision-service scenario (see our ). LIME is a widely known algorithm for detecting which input features were mostly responsible for the output of classifiers/regressors/etc. One key aspect in LIME is to meaningfully perturb the original input features in order to generate close yet reasonable copies of the input data to pass to the AI model/decision service. Such perturbed copies are used to build a surrogate (linear) model which will lead to the generation of the final feature scores. One of the difficulties of extending the original LIME implementation to the decision service scenario has been to deal with missing training data. In fact, LIME leverages training data that has been used to train a classifier (for instance) in order to decide how to perturb the numerical features. In particular it uses training data points to decide which reasonable value a perturbed numerical feature might have. Let’s imagine to have the loan approval request having a bunch of features, one of them is the number of people in the family of the person making the loan request. Of course valid numbers for such a value would be 1, 2, 3, 4, etc., not -1 nor 0.123 or 10000. In a common machine learning scenario by passing through existing values of such a feature within the training set, it would be clear that good values would be integers bigger than 1, and rarely bigger than 20. However in the decision service scenario we cannot make any such assumption, as the decision service could be anything (a proper black-box) from a rule based engine to a neural network. Additionally the training data, even if originally available, might not be available at the time when the explanation is requested. In order to address this concern the TrustyAI implementation of LIME has originally started with a simple solution: to generate new values for a given numerical feature, sample values from a standard normal distribution centered around the original feature value. For example, if the original feature value were 3 (in red), we could sample points below the bell curve in the following graph: Here the sampled values (in white) would have values of 2.7, 3.2 and 3.3. The problem with these samples is that they are not meaningful for "numbers of people in the family", there are either 2 or 3, not 2.7 persons. While training data might not be available, decision services are often used over time on a number of different inputs. We could use those past predictions and a technique from statistics called to calculate more accurate parameters for the Gaussian distribution. With bootstrap you sample (with replacement) many times to obtain statistical measures like mean, standard deviation over bootstrapped samples that can be used to generate a better suited normal distribution. This way we obtain more samples, some of them might be meaningful (red ones), some of them might not. This partially solves the problem since many (white) points are still hardly likely in reality. In order to filter out the bad points, we observe that the decision service is more confident when it make predictions with "likely" inputs. So, given the samples generated with bootstrap, we calculate the confidence of the decision service (regardless of the actual decision output). The confidence defines how much "confident" is the service in the output decision. So we plot the confidence of inputs having each of the generated data points. We can pick an area in the plot where confidence is above high (e.g. above the mean confidence value), and only pick those samples whose confidence fall in that area. This leads to less but more likely points, which in turn will generate more pertinent perturbations and therefore better explanations for the decision service at hand. In our example the final list of generated samples still contains a couple of unlikely points (3.9, 4.1) while the other points are likely values for our "number of persons in the family" feature (0, 1, 3, 5, 6). If you’re curious about the technical details of the implementation you can check the related within the kogito-apps repository. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZjJ4BMokKIM" height="1" width="1" alt=""/&gt;</content><dc:creator>Tommaso Teofili</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/effective-data-generation-for-explaining-decision-services.html</feedburner:origLink></entry><entry><title>Test pull requests with the Try in Web IDE GitHub action</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/U7jhr4yP12A/test-pull-requests-try-web-ide-github-action" /><author><name>David Kwon</name></author><id>773af125-ce7e-42db-a451-ac6f9ac9e45a</id><updated>2021-10-07T07:00:00Z</updated><published>2021-10-07T07:00:00Z</published><summary type="html">&lt;p&gt;Web-based, cloud-hosted integrated development environments (IDEs) can make development and collaboration more efficient if they are well integrated with version control. This article shows you how to use the &lt;a href="https://github.com/marketplace/actions/try-in-web-ide"&gt;Try in Web IDE GitHub action&lt;/a&gt; to test and comment on pull requests in your browser-based IDE. Examples are based on &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces&lt;/a&gt; running in the &lt;a href="https://developers.redhat.com/developer-sandbox/ide"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: CodeReady Workspaces is Red Hat's supported version of &lt;a href="https://github.com/eclipse/che"&gt;Eclipse Che&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The Try in Web IDE GitHub action&lt;/h2&gt; &lt;p&gt;The Try in Web IDE GitHub action makes it easy to try a pull request in your browser-based developer environment. The GitHub action listens to pull request events and provides a comment or status check with a link. When you click the link, it opens the branch in a new web IDE workspace.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/eclipse/che-docs"&gt;Eclipse Che documentation&lt;/a&gt; repository has recently integrated this GitHub action into its workflow. You can view a recent pull request to try it yourself. Figure 1 shows a pull request comment created by the GitHub action. Clicking the badge opens a new workspace to try the pull request in the web browser.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="The Try in Web IDE Github action creates a pull request comment." data-entity-type="file" data-entity-uuid="ffbbaaff-80ee-4416-a093-ff11ca214597" src="https://developers.redhat.com/sites/default/files/inline-images/comment.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Clicking the badge opens a web IDE workspace for testing the pull request.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 2 shows a status check done by the GitHub action. Clicking the &lt;strong&gt;Details&lt;/strong&gt; link opens a new workspace to try the pull request in the web browser.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A pull request status check." data-entity-type="file" data-entity-uuid="f798b6d4-7937-47bf-886e-916b65b567e6" src="https://developers.redhat.com/sites/default/files/inline-images/status_check.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Clicking the Details link opens a web IDE workspace for testing the pull request.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 3 shows the workspace created in CodeReady Workspaces in the Developer Sandbox. This is the web IDE workspace that opens in the web browser when you click either the badge from Figure 1 or the link from Figure 2. From here, you can try the pull request and test its correctness.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Opening a pull request branch with CodeReady Workspaces on the developer sandbox." data-entity-type="file" data-entity-uuid="b4328454-2e91-4a5d-a7bb-b8b167091f78" src="https://developers.redhat.com/sites/default/files/inline-images/crw_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: Try the pull request and test its correctness.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;GitHub actions in CodeReady Workspaces&lt;/h2&gt; &lt;p&gt;This section shows you how to add the Try in Web IDE GitHub action to your GitHub repository's workflow. We'll integrate the GitHub actions with CodeReady Workspaces in the Developer Sandbox.&lt;/p&gt; &lt;h3&gt;Prerequisites and setup&lt;/h3&gt; &lt;p&gt;You will need a Red Hat account to use CodeReady Workspaces in the Developer Sandbox. Navigate to &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, register for a free account, and launch your Developer Sandbox environment. Note that you must verify your identity with a phone number.&lt;/p&gt; &lt;p&gt;You also need an account on GitHub and a repository where you can integrate GitHub actions with your sandbox.&lt;/p&gt; &lt;p&gt;Finally, you should have a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.11/html/end-user_guide/authoring-devfiles_crw#authoring-devfiles-version-1_crw"&gt;devfile&lt;/a&gt; in the root of your GitHub repository. We'll use the devfile very shortly.&lt;/p&gt; &lt;h3&gt;Step 1: Create the GitHub workflow file&lt;/h3&gt; &lt;p&gt;In your GitHub repository, create a &lt;code&gt;.github/workflows&lt;/code&gt; directory if it does not exist already. Then, create a file named &lt;code&gt;example.yml&lt;/code&gt; in &lt;code&gt;.github/workflows&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;name: Try in Web IDE example on: pull_request_target: # Triggers workflow on pull request open types: [opened] jobs: add-link: runs-on: ubuntu-20.04 steps: - name: Web IDE Pull Request Check id: try-in-web-ide uses: redhat-actions/try-in-web-ide@v1 with: # GitHub action inputs # required github_token: ${{ secrets.GITHUB_TOKEN }} # optional - defaults to true add_comment: true # optional - defaults to true add_status: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file defines a workflow named &lt;code&gt;Try in Web IDE example&lt;/code&gt;, with a job that runs the &lt;code&gt;v1&lt;/code&gt; version of the Try in Web IDE GitHub action. The workflow is triggered on the &lt;a href="https://docs.github.com/en/actions/reference/events-that-trigger-workflows"&gt;pull_request_target&lt;/a&gt; event on the &lt;code&gt;opened&lt;/code&gt; activity type.&lt;/p&gt; &lt;h3&gt;Step 2: Configure the GitHub workflow file&lt;/h3&gt; &lt;p&gt;You can further configure the workflow defined in &lt;code&gt;example.yml&lt;/code&gt; to fit your needs. Consider adding more activity types within the &lt;code&gt;on.pull_request_target.types&lt;/code&gt; field. Alongside the &lt;code&gt;opened&lt;/code&gt; event, other events that you might find useful are &lt;code&gt;reopened&lt;/code&gt; (which is triggered when the pull request is reopened) and &lt;code&gt;synchronize&lt;/code&gt; (which is triggered when the pull request's tracking branch synchronizes with its source branch). The new types are added in the following code snippet:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;... on: pull_request_target: # Add multiple activity types types: [opened, reopened, synchronize] ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;add_comment&lt;/code&gt; and &lt;code&gt;add_status&lt;/code&gt; GitHub action inputs can also be configured to customize whether the comment or status check is created in the pull request. For example, the following code snippet disables pull request comments:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; ... with: github_token: ${{ secrets.GITHUB_TOKEN }} add_comment: false add_status: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The full table of inputs is available in the Try in Web IDE GitHub action &lt;a href="https://github.com/marketplace/actions/try-in-web-ide#action-inputs"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Step 3: Create a devfile&lt;/h3&gt; &lt;p&gt;To define the development environment of the web IDE workspace, creating a devfile in the root of the repository is highly recommended. Configuring a devfile ensures that the workspace contains everything you need to effectively try and test the pull request, such as plug-ins, development commands, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; objects, and other aspects of the environment setup.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;New to devfiles?&lt;/strong&gt; See the article, &lt;a href="https://developers.redhat.com/blog/2019/12/09/codeready-workspaces-devfile-demystified"&gt;CodeReady Workspaces devfile, demystified&lt;/a&gt; for an introduction to defining devfiles.&lt;/p&gt; &lt;p&gt;For example, specifying the Red Hat Java plug-in in the devfile provides features that &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; developers use (for things like debugging, code completion, and so on) within the web IDE workspace.&lt;/p&gt; &lt;p&gt;Here is an example of a minimal &lt;code&gt;devfile.yml&lt;/code&gt; file for a Java project:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: 1.0.0 metadata: name: project-dev-environment projects: - name: project-name-here source: type: git location: 'GITHUB REPOSITORY URL HERE' components: - type: chePlugin id: redhat/java/latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This devfile defines the project name and source location, as well as the Java plug-in. Many more &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_workspaces/2.11/html/end-user_guide/authoring-devfiles_crw#adding-components-to-a-devfile_crw"&gt;components&lt;/a&gt; can be added in a devfile to fine-tune the development environment for your specific project.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;After you have completed the steps in this article, creating a new pull request will trigger the Try in Web IDE GitHub action and create a comment, status check, or both, depending on how you've configured the action inputs. With a Red Hat account, you can now try pull requests in a web-based IDE with the click of a link.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/07/test-pull-requests-try-web-ide-github-action" title="Test pull requests with the Try in Web IDE GitHub action"&gt;Test pull requests with the Try in Web IDE GitHub action&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/U7jhr4yP12A" height="1" width="1" alt=""/&gt;</summary><dc:creator>David Kwon</dc:creator><dc:date>2021-10-07T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/07/test-pull-requests-try-web-ide-github-action</feedburner:origLink></entry><entry><title type="html">Codeanywhere adventures - Creating your first container project (part 2)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Rxq-63qYUYE/codeanywhere-adventures-creating-your-first-container-project-part2.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/mgad4uEIJw4/codeanywhere-adventures-creating-your-first-container-project-part2.html</id><updated>2021-10-07T05:00:00Z</updated><content type="html">In the , we introduced the world of , a cloud IDE and container development experience all available in just your browser.  Are you ready for some more amazing, easy to use, developer tooling that requires not a single tooling installation and no configuration?  That's what the team at  are promising us when I stumbled on their website last week. They "...don't require you to engage in complex installations and configuration setups. Simply access our in-browser IDE for everything you need to build amazing websites in a productive and more developer-friendly way." In part two of this series, we'll get hands-on creating our first Java container project in . From , you should be logged in and in your to get started. From here we'll be creating our first Java container project. THE DASHBOARD After logging in to the site, you can  view. It presents the view to get started and reminds you that you have not yet created your first container. When you create a container you are then staring your first development project, that's end-to-end based on language type selection which will generate a project in your IDE and link that directly to a container to be deployed when you are ready. Let's select a Java container project by clicking on CREATE CONTAINER. We can select the type of project (aka container) we want to use, in our case Java, by scrolling down the list of available containers. We won't select 'always on' check box as we don't need that kind of service for our development testing. Finally we give it a name, PROCESS AUTOMATION TOOLING, and click on CREATE button. Now the magic starts to happen as the project is created for you, a container is spun up, and linked to your  browser-based IDE. It's pretty fast and you might have missed the fact that the IDE is launched with your Java project layout in a new tab in your browser. Also, back in the tab hosting the Dashboard view, you'll find a new container has been listed that you created. Now let's look closer at the  IDE tab where we find an empty workspace that is waiting for you to pull in the project or start setting up one yourself. Also they were so kind as to present a GETTING STARTED tab with the essential details of the development tooling stack available to you in this container, the resource allocation available to you, how to remotely access the running container through SSH, and the URL's for accessing the deployed project once you develop something. Even more interesting, you've been provided a panel at the bottom of the IDE which is already logged into the running container, saving you the trouble. Very slick indeed! Now the  configuration settings allow you to connect any manner of existing tooling such as Github, Bitbucket, Google, OneDrive, to name just a few, but I am a Gitlab user so I'm going to use that console at the bottom of my IDE tab and clone my project into the workspace: $ git clone https://gitlab.com/bpmworkshop/rhpam-install-demo.git You see the project is brought into the IDE and automatically recognised by the EXPLORER view of our workspace. This completes part two of the  adventures, where we got started with your first browser based Java project with a fully hosted container experience.  Next up, part three takes us through the installation and deployment of the process automation tooling for developers.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Rxq-63qYUYE" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/mgad4uEIJw4/codeanywhere-adventures-creating-your-first-container-project-part2.html</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 4.1.5 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/74mhx-au4NA/eclipse-vert-x-4-1-5" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-5</id><updated>2021-10-07T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.5 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/74mhx-au4NA" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-4-1-5</feedburner:origLink></entry><entry><title>How to connect to Red Hat Data Grid without SSL</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WszNco5lnA4/how-connect-red-hat-data-grid-without-ssl" /><author><name>Varsha Sharma</name></author><id>3fa3c260-b392-4572-b8e3-1f97b0a5ad2e</id><updated>2021-10-06T07:00:00Z</updated><published>2021-10-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; is an in-memory data service you can use to speed up your applications. &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt; (SSO) provides a convenient way to connect to Data Grid. Normally, SSO is used with Secure Sockets Layer (SSL), as I explained in my &lt;a href="https://developers.redhat.com/blog/2021/04/23/integrate-red-hat-data-grid-and-red-hats-single-sign-on-technology-on-red-hat-openshift"&gt;previous article&lt;/a&gt;. But if this layer of security is not needed, you can use SSO between your application and Red Hat Data Grid without SSL. This article shows how to integrate Data Grid and SSO into &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; without SSL.&lt;/p&gt; &lt;p&gt;This integration of Data Grid and SSO with SSL communication disabled is suitable for test scenarios, where it can be used to set up an environment quickly and carry out tests to understand how Data Grid can be used as a remote store with SSO.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: All the cross-site replication use cases between Red Hat Data Grid and Red Hat's single sign-on technology are in &lt;a href="https://access.redhat.com/articles/2342861#Comp_7_5"&gt;tech preview&lt;/a&gt;. Thus, the material in this article does not make use of a cross-site replication use case and is not recommended for production use. This article employs Data Grid as a remote store with SSO, which also requires proper performance tuning and would require a support exception in order to engage Red Hat technical support.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;The minimum versions for the technologies used in this article follow:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The current version of the &lt;a href="https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html"&gt;oc command-line interface (CLI)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Red Hat Data Grid 8.2&lt;/li&gt; &lt;li&gt;Red Hat's single sign-on technology 7.5&lt;/li&gt; &lt;li&gt;Red Hat OpenShift 4.6&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Deploying the data grid&lt;/h2&gt; &lt;p&gt;You can use Red Hat Data Grid by installing the Data Grid Operator from Red Hat's &lt;a href="https://operatorhub.io/"&gt;OperatorHub.io&lt;/a&gt;. The process is similar to the one described in the "&lt;a href="https://developers.redhat.com/blog/2021/04/23/integrate-red-hat-data-grid-and-red-hats-single-sign-on-technology-on-red-hat-openshift#setting_up_data_grid"&gt;Setting up Data Grid&lt;/a&gt;"" section of my previous article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc project dg $ oc get po NAME READY STATUS RESTARTS AGE infinispan-operator-new-deploy-68fbd6d495-9n9lt 1/1 Running 0 58s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Creating an Infinispan cluster&lt;/h2&gt; &lt;p&gt;Installing the Data Grid Operator leads to the creation of an &lt;a href="https://infinispan.org/"&gt;Infinispan&lt;/a&gt; cluster. We are not using SSL in our setup, so you must disable SSL by configuring the Infinispan cluster to be created with a &lt;code&gt;endpointEncryption&lt;/code&gt; type of &lt;code&gt;none&lt;/code&gt; under the &lt;code&gt;security&lt;/code&gt; entry:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: rhsso-infinispan namespace: dg spec: security: endpointEncryption: type: None service: type: DataGrid replicas: 2&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Creating the Data Grid route&lt;/h2&gt; &lt;p&gt;You need to create an OpenShift route to the Data Grid console in order to get access to the console. You can do so by following these menu items in the OpenShift console:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Networking→Routes→Create Route→Name&lt;/strong&gt; (a unique name for the route within the project)&lt;strong&gt;→Hostname&lt;/strong&gt; (optional)&lt;strong&gt;→Service&lt;/strong&gt; (select the service, which has the same name as the Infinispan cluster)&lt;strong&gt;→Target Port &lt;/strong&gt;(11222)&lt;strong&gt;→Create&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You now have a route without TLS (the modern implementation of SSL). View the route as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD dg1 dg1-dg.apps.varsha.lab.upshift.rdu2.redhat.com rhsso-infinispan infinispan None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get access to the console via the route, you need to retrieve the developer credentials from the operator generated by the secret:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get secret rhsso-infinispan-generated-secret \ -o jsonpath="{.data.identities\.yaml}" | base64 --decode&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Creating Infinispan caches&lt;/h2&gt; &lt;p&gt;Caches required by SSO are created in Data Grid's Infinispan cluster. The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.5/html-single/server_installation_and_configuration_guide#cache"&gt;Infinispan caches&lt;/a&gt; on the Red Hat Single Sign-On side use a &lt;a href="https://infinispan.org/docs/stable/titles/configuring/configuring.html#remote_cache_store"&gt;remoteStore&lt;/a&gt; configuration (which in turn uses the Hot Rod protocol to store data on Infinispan clusters) to offload data to a Data Grid cluster. In this use case, the Data Grid cluster is in a separate namespace that replicates the offloaded data from SSO to the Infinipan cluster to ensure that the data is backed up.&lt;/p&gt; &lt;p&gt;You should create the following caches:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;sessions&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;offlineSessions&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;clientSessions&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;offlineClientSessions&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;loginFailures&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;actionTokens&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;work&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;SSO uses a separate Infinispan cache called &lt;code&gt;authenticationSessions&lt;/code&gt; to save data during each user's authentication. Requests from this cache usually involve only a browser and the SSO server, not the application. Here you can rely on sticky sessions and you don't have to replicate the &lt;code&gt;authenticationSessions&lt;/code&gt; cache content to your Data Grid cluster.&lt;/p&gt; &lt;p&gt;Caches can be created via the operator using the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.2/guide/184a8e41-7cd2-4aa8-a389-be8e9ecab3a7"&gt;Cache custom resource&lt;/a&gt; (CR). This feature is in tech preview, and therefore is not ready for production use. This CR also has some limitations. For example, the Cache CR provides a one-way mapping. Therefore, the CR cannot be edited after its creation, and deleting a CR doesn't update the server. If you want to change the configuration after creating the CR, you must tear down the Infinispan cluster and start over.&lt;/p&gt; &lt;p&gt;Caches can also be created through the CLI or the Data Grid console. To create a cache via the Data Grid console, follow these menu items:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Access the Data Grid Console&lt;/strong&gt; (access via route)&lt;strong&gt;→Data Container→Create Cache→Provide Cache Name→Provide configuration&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A work cache configuration would look like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;infinispan&gt;&lt;cache-container&gt;&lt;replicated-cache name="work" statistics="true" mode="SYNC" start="EAGER"&gt;&lt;transaction mode="NONE" locking="PESSIMISTIC"/&gt;&lt;locking acquire-timeout="0" /&gt;&lt;/replicated-cache&gt;&lt;/cache-container&gt;&lt;/infinispan&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similar configurations are created when you create the &lt;code&gt;sessions&lt;/code&gt;, &lt;code&gt;offlineSessions&lt;/code&gt;, &lt;code&gt;clientSessions&lt;/code&gt;, &lt;code&gt;offlineClientSessions&lt;/code&gt;, &lt;code&gt;loginFailures&lt;/code&gt;, and &lt;code&gt;actionTokens&lt;/code&gt; caches.&lt;/p&gt; &lt;h2&gt;Deploying SSO in a different namespace&lt;/h2&gt; &lt;p&gt;To deploy SSO, follow the steps in the "&lt;a href="https://developers.redhat.com/blog/2021/04/23/integrate-red-hat-data-grid-and-red-hats-single-sign-on-technology-on-red-hat-openshift#deploying_the_sso_image"&gt;Deploying the SSO image&lt;/a&gt;" section of my previous article. Then, make two changes to the &lt;code&gt;sso-extensions.cli&lt;/code&gt; file. First, put the Data Grid service name in the outbound socket binding:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Format of host : $DG_SERVICE_NAME.NAMESPACE.svc.cluster.local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also, add the following Hot Rod property:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;infinispan.client.hotrod.use_ssl=false&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;sso-extension.cli&lt;/code&gt; configuration file should have the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ embed-server --std-out=echo --server-config=standalone-openshift.xml batch # Add the org.keycloak.keycloak-model-infinispan module to the keycloak cache container in the Infinispan subsystem. /subsystem=infinispan/cache-container=keycloak:write-attribute(name=module,value=org.keycloak.keycloak-model-infinispan) # Create a socket binding that points to Data Grid cluster in different namespace /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=remote-cache/:add(host=rhsso-infinispan1.rdrhsso.svc.cluster.local,port=${remote.cache.port:11222},fixed-source-port=true) run-batch batch # Work cache is created as a replicated cache. The work cache itself does not cache any real data. It is used only for sending invalidation messages between cluster nodes /subsystem=infinispan/cache-container=keycloak/replicated-cache=work/store=remote:add(cache=work,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,remoteStoreSecurityEnabled=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) # Other caches can be created as a distributed cache /subsystem=infinispan/cache-container=keycloak/distributed-cache=sessions/store=remote:add(cache=sessions,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) /subsystem=infinispan/cache-container=keycloak/distributed-cache=offlineSessions/store=remote:add(cache=offlineSessions,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) /subsystem=infinispan/cache-container=keycloak/distributed-cache=clientSessions/store=remote:add(cache=clientSessions,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) /subsystem=infinispan/cache-container=keycloak/distributed-cache=offlineClientSessions/store=remote:add(cache=offlineClientSessions,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,/var/run/secrets/kubernetes.iopurge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) /subsystem=infinispan/cache-container=keycloak/distributed-cache=loginFailures/store=remote:add(cache=loginFailures,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) /subsystem=infinispan/cache-container=keycloak/distributed-cache=actionTokens/store=remote:add(cache=actionTokens,remote-servers=[remote-cache],fetch-state=false,passivation=false,preload=false,purge=false,shared=true,properties={rawValues=true,marshaller=org.keycloak.cluster.infinispan.KeycloakHotRodMarshallerFactory,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_password=NsldmxyQiVlh6kZY,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.auth_server_name=infinispan,infinispan.client.hotrod.protocol_version=2.9,infinispan.client.hotrod.use_auth=true}) run-batch reload quit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To finish the procedure, follow steps 8 through 10 in the "&lt;a href="https://developers.redhat.com/blog/2021/04/23/integrate-red-hat-data-grid-and-red-hats-single-sign-on-technology-on-red-hat-openshift#deploying_the_sso_image"&gt;Deploying the SSO image&lt;/a&gt;" section of my previous article. These steps create a config map named &lt;code&gt;jboss-cli&lt;/code&gt; using the &lt;code&gt;sso-extensions.cli&lt;/code&gt; file and mount that config map as a volume.&lt;/p&gt; &lt;p&gt;You can verify your integration using the tests in the "&lt;a href="https://developers.redhat.com/blog/2021/04/23/integrate-red-hat-data-grid-and-red-hats-single-sign-on-technology-on-red-hat-openshift#verifying_the_integration"&gt;Verifying the integration&lt;/a&gt;" section of my previous article.&lt;/p&gt; &lt;p&gt;Here Data Grid and SSO are deployed in different namespaces, whereas the previous article deployed both in the same namespace.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has shown that Red Hat Data Grid and Red Hat's single sign-on technology are flexible and can work together in different ways to provide high-speed computing. Try out Data Grid today.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/06/how-connect-red-hat-data-grid-without-ssl" title="How to connect to Red Hat Data Grid without SSL"&gt;How to connect to Red Hat Data Grid without SSL&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WszNco5lnA4" height="1" width="1" alt=""/&gt;</summary><dc:creator>Varsha Sharma</dc:creator><dc:date>2021-10-06T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/06/how-connect-red-hat-data-grid-without-ssl</feedburner:origLink></entry><entry><title type="html">Quarkus 2.3.0.Final released - Dev Service for Neo4J, Logging with Panache, Testing CLI applications and much more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0uGEvAKNvfU/" /><author><name>Alexey Loubyansky</name></author><id>https://quarkus.io/blog/quarkus-2-3-0-final-released/</id><updated>2021-10-06T00:00:00Z</updated><content type="html">Today, we release Quarkus 2.3.0.Final which includes a lot of refinements and improvements and some new features: Dev Service for Neo4J Logging with Panache Testing support for CLI applications MongoDB Liquibase extension Support for Hibernate ORM interceptors Migration Guide To migrate from 2.2, please refer to our migration guide. What’s...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0uGEvAKNvfU" height="1" width="1" alt=""/&gt;</content><dc:creator>Alexey Loubyansky</dc:creator><feedburner:origLink>https://quarkus.io/blog/quarkus-2-3-0-final-released/</feedburner:origLink></entry><entry><title>Printf-style debugging using GDB, Part 1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/fAUSoz0N0s0/printf-style-debugging-using-gdb-part-1" /><author><name>Kevin Buettner</name></author><id>f51c0dec-7caf-4775-ba1d-f910d0857017</id><updated>2021-10-05T07:00:00Z</updated><published>2021-10-05T07:00:00Z</published><summary type="html">&lt;p&gt;Programmers often debug software by adding print statements to source code. Knowing that a certain point in the program has been reached can be immensely helpful. It's also useful to print values of variables at various points during program execution. An obvious drawback of this technique is the need to change source code, both to add the print statements and later to remove or disable them after the bug has been fixed. Adding new code can potentially introduce new bugs, and if you've added many print statements, you might forget to remove some of them when cleaning up after debugging.&lt;/p&gt; &lt;p&gt;You can use the popular &lt;a href="https://www.gnu.org/software/gdb/"&gt;GNU Project Debugger&lt;/a&gt; (GDB) to perform the same style of debugging for various programming languages, especially &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt;, without changing source files. This article is the first of a series describing how to use GDB to add print statements to your C and C++ code. We'll start with some basics and move through more advanced ways to call program-defined functions that display data.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To use the techniques described in this article, you need to satisfy the following prerequisites:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You must have a C/C++ compiler such as &lt;a href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; or &lt;a href="https://clang.llvm.org/"&gt;Clang&lt;/a&gt; installed on your development machine.&lt;/li&gt; &lt;li&gt;Likewise, you will need GDB installed on your development machine.&lt;/li&gt; &lt;li&gt;The program that you wish to debug needs to be compiled with debugging information. When using either the &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;clang&lt;/code&gt; command, add the &lt;code&gt;-g&lt;/code&gt; option to enable debugging information.&lt;/li&gt; &lt;li&gt;The program—or at least the source files on which you wish to use GDB's &lt;code&gt;dprintf&lt;/code&gt; command—should be compiled without optimization.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Regarding the last point, you can disable optimization either by removing any existing &lt;code&gt;-O&lt;/code&gt;, -&lt;code&gt;O2&lt;/code&gt;, etc., options from the set of compiler flags (for example, &lt;code&gt;CFLAGS&lt;/code&gt; or &lt;code&gt;CXXFLAGS&lt;/code&gt;) or by adding either &lt;code&gt;-O0&lt;/code&gt; or &lt;code&gt;-Og&lt;/code&gt; as the very last optimization option. When you run &lt;code&gt;gcc&lt;/code&gt; without specifying any optimization options, &lt;code&gt;-O0&lt;/code&gt; is used as the default. When the program is being compiled with a lot of options, it may be easier to simply append either &lt;code&gt;-O0&lt;/code&gt; or &lt;code&gt;-Og&lt;/code&gt; to the list of compiler options, because the final optimization option overrides any previous optimization options. The &lt;code&gt;-O0&lt;/code&gt; option is slightly different from the &lt;code&gt;-Og&lt;/code&gt; option, because the latter enables some optimizations that won't affect the debugging experience whereas &lt;code&gt;-O0&lt;/code&gt; disables all optimizations.&lt;/p&gt; &lt;p&gt;Note, too, that the latter two points might require you to recompile and relink your program with a debugging option enabled and with optimization disabled.&lt;/p&gt; &lt;p&gt;The techniques presented here might also work for &lt;em&gt;some&lt;/em&gt; optimized code. In recent years, when &lt;code&gt;gcc&lt;/code&gt; is used, the quality of the debugging information has improved greatly for optimized code; however, there are still cases where values of variables are unavailable or possibly even incorrect. Programmers can avoid these problems by disabling optimization.&lt;/p&gt; &lt;h2&gt;Example code&lt;/h2&gt; &lt;p&gt;This article demonstrates the use of GDB to add printf-style output for a little function named &lt;code&gt;insert&lt;/code&gt;. This function is from a small program that I wrote for pedagogical purposes. The program, which is a little over 100 lines long, is contained in a single source file named &lt;code&gt;tree.c&lt;/code&gt; that is available from my &lt;a href="https://github.com/KevinBuettner/tree.c"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following &lt;code&gt;insert&lt;/code&gt; function inserts a node into a binary search tree. Note the line numbers, which we'll use later in the article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; 37 struct node * 38 insert (struct node *tree, char *data) 39 { 40 if (tree == NULL) 41 return alloc_node (NULL, NULL, data); 42 else 43 { 44 int cmp = strcmp (tree-&gt;data, data); 45 46 if (cmp &gt; 0) 47 tree-&gt;left = insert (tree-&gt;left, data); 48 else if (cmp &lt; 0) 49 tree-&gt;right = insert (tree-&gt;right, data); 50 return tree; 51 } 52 } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;main&lt;/code&gt; function contains calls to &lt;code&gt;insert&lt;/code&gt; plus additional calls to functions for printing the tree. Note the line numbers here:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; 96 struct node *tree = NULL; 97 98 tree = insert (tree, "dog"); 99 tree = insert (tree, "cat"); 100 tree = insert (tree, "wolf"); 101 tree = insert (tree, "javelina"); 102 tree = insert (tree, "gecko"); 103 tree = insert (tree, "coyote"); 104 tree = insert (tree, "scorpion"); 105 106 print_tree_flat (tree); 107 printf ("\n"); 108 print_tree (tree); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I compiled &lt;code&gt;tree.c&lt;/code&gt; for use with GDB using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gcc -o tree -g tree.c&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-g&lt;/code&gt; option places debugging information in the binary. Also, the program is compiled without optimization.&lt;/p&gt; &lt;h2&gt;Using GDB for printf-style output&lt;/h2&gt; &lt;p&gt;With the properly compiled binary on your system, you can simulate print statements in GDB.&lt;/p&gt; &lt;h3&gt;Debugging with GDB&lt;/h3&gt; &lt;p&gt;We can use the &lt;code&gt;gdb&lt;/code&gt; command to debug the example program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb ./tree&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command starts by printing a copyright message along with legal and help information. If you wish to silence that output, add the &lt;code&gt;-q&lt;/code&gt; option to the &lt;code&gt;gdb&lt;/code&gt; command line. This is what output should look like when using the &lt;code&gt;-q&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ gdb -q ./tree Reading symbols from ./tree... (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you also see the message &lt;code&gt;(No debugging symbols found in ./tree)&lt;/code&gt;, it means that you did not enable the generation of debugging information during the compilation and linking of the program. If this is the case, use GDB's &lt;code&gt;quit&lt;/code&gt; command to exit GDB and fix the problem by recompiling with the &lt;code&gt;-g&lt;/code&gt; option.&lt;/p&gt; &lt;h3&gt;Virtual print statements&lt;/h3&gt; &lt;p&gt;We'll now use GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Dynamic-Printf.html#Dynamic-Printf"&gt;dprintf command&lt;/a&gt; to place a special kind of breakpoint that simulates the addition of a comparable &lt;code&gt;printf()&lt;/code&gt; statement to the source code. We'll place virtual print statements on lines 41, 47, and 49:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) dprintf 41,"Allocating node for data=%s\n", data Dprintf 1 at 0x401281: file tree.c, line 41. (gdb) dprintf 47,"Recursing left for %s at node %s\n", data, tree-&gt;data Dprintf 2 at 0x4012b9: file tree.c, line 47. (gdb) dprintf 49,"Recursing right for %s at node %s\n", data, tree-&gt;data Dprintf 3 at 0x4012de: file tree.c, line 49. (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first &lt;code&gt;dprintf&lt;/code&gt; command shown for line &lt;code&gt;41&lt;/code&gt; is roughly equivalent to adding three lines of code near lines 40 and 41:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; if (tree == NULL) { /* DEBUG - delete later. */ printf ("Allocating node for data=%s\n", data); /* DEBUG - delete later. */ return alloc_node (NULL, NULL, data); } /* DEBUG - delete later. */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that, when adding a call to &lt;code&gt;printf()&lt;/code&gt; in the traditional fashion, three lines of code would need to be added in this particular place. (If you added the &lt;code&gt;printf()&lt;/code&gt; without the curly braces, the &lt;code&gt;if&lt;/code&gt; statement would execute only the &lt;code&gt;printf()&lt;/code&gt;, and the &lt;code&gt;return alloc_node&lt;/code&gt; statement would no longer be conditionally executed—instead, it would &lt;em&gt;always&lt;/em&gt; be executed.)&lt;/p&gt; &lt;p&gt;As indicated by the comments, you would need to delete these added lines later when debugging is done (although the added braces are actually fine to leave in place). If you add lots of debugging statements to your code, you might forget to delete some of them when debugging is complete. As noted earlier, this is a distinct advantage of using GDB's &lt;code&gt;dprintf&lt;/code&gt; command: No source code is modified, so subtle bugs won't be introduced when adding the print statement; there's also no need to remember all the places where a print statement was added when cleaning up after debugging.&lt;/p&gt; &lt;h3&gt;Run the program&lt;/h3&gt; &lt;p&gt;Use GDB's &lt;code&gt;run&lt;/code&gt; command to run your program. Once the command is issued, GDB output and program output appear mixed together in the terminal used for the GDB session. Here's an example running our tree program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt; (gdb) run Starting program: /home/kev/ctests/tree Allocating node for data=dog Recursing left for cat at node dog Allocating node for data=cat Recursing right for wolf at node dog Allocating node for data=wolf Recursing right for javelina at node dog Recursing left for javelina at node wolf Allocating node for data=javelina Recursing right for gecko at node dog Recursing left for gecko at node wolf Recursing left for gecko at node javelina Allocating node for data=gecko Recursing left for coyote at node dog Recursing right for coyote at node cat Allocating node for data=coyote Recursing right for scorpion at node dog Recursing left for scorpion at node wolf Recursing right for scorpion at node javelina Allocating node for data=scorpion cat coyote dog gecko javelina scorpion wolf cat coyote dog gecko javelina scorpion wolf [Inferior 1 (process 306927) exited normally] (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this display, the user typed the &lt;code&gt;run&lt;/code&gt; command at the &lt;code&gt;(gdb)&lt;/code&gt; prompt. The rest of the lines are output either from GDB or from the program. The only program output occurs towards the end, starting with the line "cat coyote dog..." and finishing with the line "wolf." Lines starting with either "Recursing" or "Allocating" were output by the &lt;code&gt;dprintf&lt;/code&gt; commands established earlier. It's important to understand that, by default, these lines were output by GDB. This is different from traditional printf-style debugging, and we'll look at this difference in the next article in this series. Finally, there are two lines of GDB output, the second line and the penultimate one, which show that the program is starting and exiting.&lt;/p&gt; &lt;h2&gt;Comparing dprintf and printf()&lt;/h2&gt; &lt;p&gt;There are differences and similarities between GDB's &lt;code&gt;dprintf&lt;/code&gt; command and the C-language &lt;code&gt;printf()&lt;/code&gt; function:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;dprintf&lt;/code&gt; command does not use parentheses to group the command's arguments.&lt;/li&gt; &lt;li&gt;The first argument of the &lt;code&gt;dprintf&lt;/code&gt; command specifies a source location at which a dynamic &lt;code&gt;printf&lt;/code&gt; statement should be placed. Output from the dynamic &lt;code&gt;printf&lt;/code&gt; is printed &lt;em&gt;prior to&lt;/em&gt; the execution of that source location. The source location may be a line number, such as &lt;code&gt;41&lt;/code&gt;, but the location will often include a filename plus a line number, such as &lt;code&gt;tree.c:41&lt;/code&gt;. The location could also be the name of a function or an instruction address in the program. For a function location, the output from the dynamic &lt;code&gt;printf&lt;/code&gt; occurs prior to the first executable line of the function. When the location is an instruction address, output occurs before the instruction at that address is executed.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;dprintf&lt;/code&gt; command creates a special kind of breakpoint. It is only when one of these special breakpoints is hit during the program run that output is printed.&lt;/li&gt; &lt;li&gt;The format string used by &lt;code&gt;dprintf&lt;/code&gt; is the same as that used by &lt;code&gt;printf()&lt;/code&gt;. In fact, as we shall see later, the format string specified in the &lt;code&gt;dprintf&lt;/code&gt; command might be passed to a dynamically constructed call to &lt;code&gt;printf()&lt;/code&gt; in the program being debugged.&lt;/li&gt; &lt;li&gt;In both &lt;code&gt;dprintf&lt;/code&gt; and &lt;code&gt;printf()&lt;/code&gt;, comma-separated expressions follow the format string. These are evaluated and output according to the specification provided by the format string.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has offered the basics of printf-style debugging in GDB. The next article in this series takes you to a higher level of control over debugging, by showing you how to save your &lt;code&gt;dprintf&lt;/code&gt; commands and the GDB output for later use.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1" title="Printf-style debugging using GDB, Part 1"&gt;Printf-style debugging using GDB, Part 1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/fAUSoz0N0s0" height="1" width="1" alt=""/&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2021-10-05T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1</feedburner:origLink></entry><entry><title type="html">WildFly 25 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/z43JPpPifIg/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/10/05/WildFly25-Final-Released/</id><updated>2021-10-05T00:00:00Z</updated><content type="html">I’m pleased to announce that the WildFly 25 Final zip is now available . The big focus during the WildFly 25 development cycle has been on support for Java SE 17 and on the related transition away from our legacy security layer and to a purely -based . More about those later, but first let’s look at new features in WildFly 25. NEW FEATURES * A new subsystem has been added that provides , allowing for the injection of the OpenTelemetry and Tracer objects from the specification, as well as implicit tracing of Jakarta REST endpoints. WildFly still provides MicroProfile OpenTracing as an alternative, but I encourage users to switch to the new OpenTelemetry subsystem. * A new subsystem has been added that , without needing to make use of the Keycloak client adapter. It is now possible to make use of other OpenID Connect providers in addition to Keycloak. * MicroProfile Health support has been updated to , a new backwards compatible release of the specification. MicroProfile Health 3.1 adds support for Kubernetes startup probes in form of a new @Startup CDI qualifier, with WildFly exposing this check at the :9990/health/started endpoint. * We’ve shipped an . This now integrates with MicroProfile Health for messages sent, and facilitates user-initiated code to push data to, and, to some extent, receive data from, Reactive Messaging streams. * The MicroProfile Reactive Messaging subsystem now supports , and provides means of getting information from Kafka on the receiving end. * You can now connect to a secure Kafka instance using the MicroProfile Reactive Messaging functionality of WildFly. For cases where you are using self-signed certificates, . * WildFly now supports . If a system property value can be found, that is returned as has happened until now. If no system property is found, the name is converted to environment property format and the value of the environment variable is checked. The conversion happens by replacing each character that is neither alphanumeric nor underscore with underscore, and then converting the name to upper case (i.e. com.acme-size becomes COM_ACME_SIZE). This feature makes it easier to reuse configuration in different deployment enviroments, particularly in cloud environments where environment variables are more readily used than system properties. * Logging related to discovery of failed JCA connections during validation . SECURITY LAYER CHANGES A key focus in WildFly 25 has been completing our migration away from the legacy security layer that dates back to JBoss AS and onto the -based introduced in WildFly 11. SE 17 does not provide packages that legacy security heavily relies upon, so the time has come to complete the transition off of legacy security. We deprecated the use of legacy security long ago and in the WildFly 25 release we have removed support for it. As part of this change you will see a number of significant changes in WildFly 25: * Our standard configuration files no longer include legacy security realms. These are the 'security-realm' elements found under the 'management' element in a standalone.xml or host.xml file, administered via the CLI at '/core-service=management/security-realm=*' addresses. The xml parsers no longer support these elements and the management API no longer provides resources at these addresses. Elytron subsystem resources are now used. * Use of the Picketbox-based security vault is no longer supported. Elytron credential stores should be used instead. * The 'org.jboss.as.security' extension and the 'security' subsystem it provides are no longer supported on servers not running in 'admin-only' mode. The extension and subystem can still be used on a WildFly 25 Domain Controller to allow it to manage hosts running earlier versions of WildFly. * The 'org.wildlfy.extension.picketlink' extension and the 'picketlink-federation' and 'picketlink-idm' subsystems it provides are no longer supported on servers not running in 'admin-only' mode. They can still be used on a WildFly 25 Domain Controller to allow it to manage hosts running earlier versions of WildFly. Note that the reason use of the legacy security and picketlink extensions is allowed on an 'admin-only' server is to allow a server with a configuration using those to boot so an administrator can then use the CLI to alter the server configuration to use Elytron. I very much encourage any of you still using legacy security in your configuration to start experimenting with WildFly 25. WILDFLY PREVIEW As I when we released WildFly 22 Alpha1, along with our traditional Jakarta EE 8 distribution we want to give our users a preview of what will be coming in WildFly as we move on to EE 9 and later. We call this distribution "WildFly Preview". The WildFly 25.0.0.Final release includes an update to WildFly Preview. Even though this is coming from a .Final tag of the WildFly codebase, WildFly Preview should always be regarded as a tech-preview/beta distribution. To learn more about WildFly Preview, see the . JAVA SE 17 SUPPORT I’m extremely pleased to be able to say that we can recommend you run WildFly 25 or WildFly Preview 25 on any of the long-term support Java SE releases, including Java SE 17. We’ve tested WildFly heavily on Java SE 8, Java SE 11 and Java SE 17. Our testing included testing WildFly Preview on SE 17 with the massive Jakarta EE 9.1 TCK. (More on that in the 'Standards Support' section below.) The most heavily tested SE options for WildFly are still SE 11 and SE 8, because both WildFly and its component library projects have so many years of testing on those versions. As I noted in my recent , it is likely that WildFly will drop support for SE 8 in one of the next few quarterly releases. Eventually the transition to Jakarta EE 10 support and the expected minimum requirement for SE 11 by some of its API projects will drive WildFly to only support SE 11 or later. As I described in that post, it’s possible this could happen as soon as WildFly 26, although I doubt that will happen and will work to avoid it. Please note that WildFly runs on Java 11 and later in classpath mode. RUNNING WILDFLY 25 WITH SE 17 One of the key differences in SE 17 versus the previous LTS SE 11 release is that the JVM will reject reflective access calls that SE 11 would only warn about, unless the JVM launch command includes JPMS configuration options to allow that access. WildFly does quite a bit of deep reflection, so part of our efforts in recent releases has been to identify the necessary JPMS settings. We have added those to our standard launch scripts, so WildFly should just work if you’re using those. The manifest file in a WildFly bootable jar will also include these settings. But some users may not be using a bootable jar or using our launch scripts to launch WildFly. For example many users use IDEs to launch WildFly and count on the IDE to provide arguments to the JVM. And IDEs may not be using the necessary settings yet. If you are launching a WildFly instance on SE 17 and aren’t using a bootable jar or our launch scripts, here are the JPMS settings you will need: * --add-exports=java.desktop/sun.awt=ALL-UNNAMED * --add-exports=java.naming/com.sun.jndi.ldap=ALL-UNNAMED * --add-opens=java.base/java.lang=ALL-UNNAMED * --add-opens=java.base/java.lang.invoke=ALL-UNNAMED * --add-opens=java.base/java.lang.reflect=ALL-UNNAMED * --add-opens=java.base/java.io=ALL-UNNAMED * --add-opens=java.base/java.security=ALL-UNNAMED * --add-opens=java.base/java.util=ALL-UNNAMED * --add-opens=java.base/java.util.concurrent=ALL-UNNAMED * --add-opens=java.management/javax.management=ALL-UNNAMED * --add-opens=java.naming/javax.naming=ALL-UNNAMED Not all uses of the server will require all of those; the that set those up include comments describing the main reason we’ve added each. It’s possible your application may do something that requires additional JPMS settings; if so you can add those to the JVM launch command by editing the 'bin/standalone.conf` or 'bin/domain.conf' file or their .bat or .ps1 variants. STANDARDS SUPPORT The standard WildFly 25.0.0 distribution is a Jakarta EE 8 compatible implementation, compatible with both the Full Platform and the Web Profile. Evidence supporting our certification is available and . The standard WildFly 25 distribution is also a compliant implementation of the MicroProfile 4.1 platform specification. The WildFly Preview distribution released today is a compatible implementation of both the Jakarta EE 9.1 Web Profile and the Full Platform. WildFly Preview has been able to demonstrate compatibility while running on both Java SE 11 and on Java SE 17! Evidence supporting our certification is available , , and . Many thanks to the folks in the Jakarta EE community who worked hard to make it possible to run the EE 9.1 TCKs on Java SE 17! Implementations being able to demonstrate compliance using an SE version that came out after the EE release did is an important step forward for Jakarta EE. GREAT COMMUNITY I want to particularly thank a couple members of the WildFly community for their efforts during the WildFly 25 dev cycle. One is Darran Lofthouse, who coordinated the WildFly 25 Beta release, and did the biggest part of the very heavy lifting related to the removal of support for the legacy security layer. Another is Boris Unckel, who has been very active this year filing issues, mentoring new contributors and doing a lot of work helping to elevate WildFly’s code quality. Thank you Darran and Boris! I’d also like to invite participants in this month’s to come say 'Hi' in the and find out about contributing to WildFly. UPCOMING CHANGES WildFly 25 was the first in a series of a few releases where we’re expecting to make some big changes in the server. I encourage you to have a look at the that I mentioned above. DOCUMENTATION The WildFly 25 documentation is available at the . The WildFly 25 management API documentation is in the . JIRA RELEASE NOTES The full list of issues resolved is available . Issues resolved in the WildFly Core 17 releases included with WildFly 25 are available . ENJOY! Thank you for your continued support of WildFly. We’d love to hear your feedback at the .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/z43JPpPifIg" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/05/WildFly25-Final-Released/</feedburner:origLink></entry><entry><title type="html">Beginners Guide to Installing Decision Management Tooling in a Local Container using Podman</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6GqVdhwu6Jw/beginners-guide-to-rhdm-local-conainter-podman.html.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/QBFd-HmZdu4/beginners-guide-to-rhdm-local-conainter-podman.html.html</id><updated>2021-10-04T05:00:00Z</updated><content type="html"> Recently the open source  announced that there was solid support for using its container tooling to replace docker on your local development machine. Ring in the joyous music and off we go to explore how we can get back to basics without the issues of licensing around the developer desktop container tooling. Note, the rest of this tutorial will be based on the current version of Podman at the time of publication, v3.3.1. The first thing you want to do is just install the Podman tooling, which is fairly painless using BREW: $ brew install podman Now you are ready to kick off the virtual machine with the proper settings to start doing something real, like adding developer decision management tooling to your local machine. You start by initialise and start the virtual machine to run Podman on: $ podman machine init --memory 6144 --disk-size 20 Extracting compressed file Image resized. If it's your first time doing this, an image will be downloaded first, but in this case a cached version of that image is just being unpacked and setup. Now it's time to start the virtual machine: $ podman machine start INFO[0000] waiting for clients... INFO[0000] listening tcp://0.0.0.0:7777 INFO[0000] new connection from to /var/folders/_y/1rjzwypx57sd677v9jzrr0nc0000gp/T/podman/qemu_podman-machine-default.sock Waiting for VM ... qemu-system-x86_64: warning: host doesn't support requested feature: CPUID.80000001H:ECX.svm [bit 2] Now once this completes, you're ready to take a swing at spinning up the developer decision management tooling by Red Hat. First we pull in this nice demo project for installing it locally or in a container: $ git clone https://gitlab.com/bpmworkshop/rhdm-install-demo.git Then go into the rhpdm-install-demo directory and examine the instructions for using this with Podman to spin up the decision management developer tooling in a container. You will note that you need to head over to  and obtain the files listed in the installs/README before you can continue.  Before you try to use Podman to build, pull, or run any images on your machine, verify that the virtual machine is running to support you: $ podman machine list NAME VM TYPE CREATED LAST UP podman-machine-default* qemu 9 minutes ago Currently running Once that's done, you can start from the root directory and build your first image: $ podman build -t rhdm-install:7.11 . ...CUT OUTPUT... STEP 24/26: USER 1000 --&gt; 2a682b0c5aa STEP 25/26: EXPOSE 9990 9999 8080 8001 --&gt; 6a1ed7c4d2e STEP 26/26: CMD ["/opt/jboss/rhdm/jboss-eap-7.3/bin/standalone.sh","-c","standalone.xml","-b", "0.0.0.0","-bmanagement","0.0.0.0"] COMMIT rhpam-install:7.11 --&gt; fbba2e41494 Successfully tagged localhost/rhdm-install:7.11 ce1e4f2a11567442c2c22c8fc2253076c463a154e6696c9fed68bd70e9f4146d This will take some time to execute every line in the provided Dockerfile, so feel free to watch or explore that file until the build process is done. You can verify that you have a new image built: $ podman image list REPOSITORY TAG IMAGE ID CREATED SIZE localhost/rhdm-install 7.11 ce1e4f2a1156 28 sec ago 2.38 GB docker.io/jbossdemocentral/developer latest b73501ac39b1 5 years ago 514 MB You can see the base image is a customised developer image that we then use to build our rhdm-install image.  The next step is to run the image: $ podman run -dt -p 8080:8080 -p 9990:9990 rhdm-install 20a92c865c4a0149779ef70b9a54852ca2168509a7dc7ce8527de015a9895ae7 This starts the image and spins up both Red Hat Enterprise Application Server and running inside of that the Red Hat Decision Management tooling. You can view this by looking up the container id, then viewing the log file to ensure the container startup is completed before using the tooling: $ podman container list CONTAINER ID IMAGE COMMAND CREATED 20a92c865c4a localhost/rhdm-install:7.11 /opt/jboss/rhdm/... 43 sec ago $ podman logs ... CUT OUTPUT OF DUMPED LOG FILE... 14:50:50,612 INFO [org.kie.workbench.common.screens.datasource.management.backend.DataSourceManagementBootstrap] (pool-30-thread-1) Initialize deployments task finished successfully. Note you can use the -f flag to attache the logs output to the console and watch the container start up. Now that it's been started successfully we can make use of the port mapping we did at podman run when we used the -p flag. Port 8080 from our localhost is now mapped through to the containers port.  You can verify this with: $ podman port 20a92c865c4a 8080/tcp -&gt; 0.0.0.0:8080 9990/tcp -&gt; 0.0.0.0:9990 Now you can access the tooling through decision central by logging in at: http://localhost:8080/decision-central The login user: erics The password: redhatdm1! You will be presented with the business central dashboard and you are ready to start automating processes using this tooling in a container on your local machine.  If you need help getting started, try this !&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6GqVdhwu6Jw" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/QBFd-HmZdu4/beginners-guide-to-rhdm-local-conainter-podman.html.html</feedburner:origLink></entry></feed>
